{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32665500",
   "metadata": {},
   "source": [
    "## NLP(Natural Language Processing)\n",
    "When we talk about data, we not only reference numbers, but also text and human speech. But how do we approach the latter? This field covers both linguistics and AI. We'll not be needed to understand linguistics in depth ie morphology, semantics, pragmatics etc. because those who came before us figure out a way to give use the tools necessary to work with human language without that hassle.\n",
    "\n",
    "So in this section will cover how to use these tools(NLTK) when our analytics involves text data(human language).\n",
    "\n",
    "We'll be using a very popular library in Natural Language Processing called NLTK(Natural Language Tool Kit).\n",
    "\n",
    "\n",
    "#### Objectives\n",
    "1. Preprocessing and Exploring Text Data.\n",
    "2. Preparing our data for machine learning(Vectorizing).\n",
    "3. Fitting machine learning models with text data(vectorized data).\n",
    "\n",
    "When working with text data, the bulk of the tasks involves preparation of the data. Unlike the normal data where the steps are pretty straightforward/rigid, in text data you have to adjust based on the nature of the data and the objectives you are aiming for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f1dde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'stole', 'the', 'stew', 'on', 'my', \"grandma's\", 'stove.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "\n",
    "sentence_1 = \"The cat steals the stew on my Grandma's stove.\"\n",
    "\n",
    "sentence_3 = \"The dog stole the stew on my gRandma's jiko.\"\n",
    "\n",
    "sentence_4 = \"The rat was stealing the stew on the grandma's gas.\"\n",
    "\n",
    "sentence_5 = \"I like eating a sandwich.\"\n",
    "sentence_5 = \"I ate burgers in the morning.\"\n",
    "sentence_5 = \"I like eating sandwiches.\"\n",
    "\n",
    "\"\"\" \n",
    "1. stemming - chopping off the ends of words eg 'sandwiches' we remove the 'es' to remain with 'sandwich'\n",
    "2. lemmatization - improved version of stemming, it considers both pos and morphology of the word eg\n",
    "                    'ate' is changed to 'eat', 'better' is changed to 'good'\n",
    "\"\"\"\n",
    "\n",
    "context_1 = 'The baseball bat flew over the fence, startling the cave bat.'\n",
    "\n",
    "sentence_2 = \"Unlike the normal data where the steps are pretty straightforward/rigid, in text data you have to adjust based on the nature of the data and the objectives you are aiming for.\"\n",
    "\n",
    "\"\"\" \n",
    "                the | cat | stole | stew | on | my | grandmas | stove | jiko | gas | dog | rat\n",
    "sentence_1       2      1      1       1    1    1      1          1       0    0      0    0\n",
    "sentence_2       2      1      1       1    1    1      1          1       0    0      0    0\n",
    "sentence_3       2      0      1       1    1    1      1          0       1    0      0    0\n",
    "sentence_4       3      0      1       1    1    0      1          0       0    1      0    1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "standardized_sentence_1 = [part.lower() for part in sentence_1.split()]\n",
    "standardized_sentence_2 = [part.lower() for part in sentence_2.split()]\n",
    "standardized_sentence_3 = [part.lower() for part in sentence_3.split()]\n",
    "standardized_sentence_4 = [part.lower() for part in sentence_4.split()]\n",
    "\n",
    "\n",
    "# standardized_sentence_1 = []\n",
    "\n",
    "# for part in sentence_1.split():\n",
    "#     standardized_sentence_1.append(part.lower())\n",
    "\n",
    "standardized_sentence_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67f940ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'stole', 'the', 'stew', 'on', 'my', 'grandmas', 'stove']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no_punc_standardized_sentence_1 = []\n",
    "\n",
    "# for part in standardized_sentence_1:\n",
    "#     no_punc_standardized_sentence_1.append(part.replace(\"'\", \"\").replace(\".\"))\n",
    "\n",
    "no_punc_standardized_sentence_1 = [part.replace(\"'\", \"\").replace(\".\", \"\") for part in standardized_sentence_1]\n",
    "no_punc_standardized_sentence_2 = [part.replace(\"'\", \"\").replace(\".\", \"\").replace(\"/\", \" \").replace(\",\", \"\") for part in standardized_sentence_2]\n",
    "no_punc_standardized_sentence_3 = [part.replace(\"'\", \"\").replace(\".\", \"\") for part in standardized_sentence_3]\n",
    "no_punc_standardized_sentence_4 = [part.replace(\"'\", \"\").replace(\".\", \"\") for part in standardized_sentence_4]\n",
    "\n",
    "no_punc_standardized_sentence_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8849d11",
   "metadata": {},
   "source": [
    "#### Vectorization\n",
    "Transforming the tokens into numerical vectors for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1ca6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#initialize the vectorizer\n",
    "counter_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3313ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cat', 'stole', 'the', 'stew', 'on', 'my', 'grandmas', 'stove']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punc_standardized_sentence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6faaf6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the|cat|stole|the|stew|on|my|grandmas|stove'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"|\".join(no_punc_standardized_sentence_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07d556de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the cat stole the stew on my grandmas stove',\n",
       " 'unlike the normal data where the steps are pretty straightforward rigid in text data you have to adjust based on the nature of the data and the objectives you are aiming for',\n",
       " 'the dog stole the stew on my grandmas jiko',\n",
       " 'the rat stole the stew on the grandmas gas']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_sentences = [\n",
    "    \" \".join(no_punc_standardized_sentence_1),\n",
    "    \" \".join(no_punc_standardized_sentence_2),\n",
    "    \" \".join(no_punc_standardized_sentence_3),\n",
    "    \" \".join(no_punc_standardized_sentence_4)\n",
    "]\n",
    "combined_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be49a86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 29,\n",
       " 'cat': 5,\n",
       " 'stole': 25,\n",
       " 'stew': 24,\n",
       " 'on': 19,\n",
       " 'my': 14,\n",
       " 'grandmas': 10,\n",
       " 'stove': 26,\n",
       " 'unlike': 31,\n",
       " 'normal': 16,\n",
       " 'data': 6,\n",
       " 'where': 32,\n",
       " 'steps': 23,\n",
       " 'are': 3,\n",
       " 'pretty': 20,\n",
       " 'straightforward': 27,\n",
       " 'rigid': 22,\n",
       " 'in': 12,\n",
       " 'text': 28,\n",
       " 'you': 33,\n",
       " 'have': 11,\n",
       " 'to': 30,\n",
       " 'adjust': 0,\n",
       " 'based': 4,\n",
       " 'nature': 15,\n",
       " 'of': 18,\n",
       " 'and': 2,\n",
       " 'objectives': 17,\n",
       " 'aiming': 1,\n",
       " 'for': 8,\n",
       " 'dog': 7,\n",
       " 'jiko': 13,\n",
       " 'rat': 21,\n",
       " 'gas': 9}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine the tokens into a sentence and then apply countvectorizer\n",
    "count_matrix = counter_vectorizer.fit_transform(combined_sentences)\n",
    "\n",
    "counter_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a92ff665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('adjust', 0),\n",
       " ('aiming', 1),\n",
       " ('and', 2),\n",
       " ('are', 3),\n",
       " ('based', 4),\n",
       " ('cat', 5),\n",
       " ('data', 6),\n",
       " ('dog', 7),\n",
       " ('for', 8),\n",
       " ('gas', 9),\n",
       " ('grandmas', 10),\n",
       " ('have', 11),\n",
       " ('in', 12),\n",
       " ('jiko', 13),\n",
       " ('my', 14),\n",
       " ('nature', 15),\n",
       " ('normal', 16),\n",
       " ('objectives', 17),\n",
       " ('of', 18),\n",
       " ('on', 19),\n",
       " ('pretty', 20),\n",
       " ('rat', 21),\n",
       " ('rigid', 22),\n",
       " ('steps', 23),\n",
       " ('stew', 24),\n",
       " ('stole', 25),\n",
       " ('stove', 26),\n",
       " ('straightforward', 27),\n",
       " ('text', 28),\n",
       " ('the', 29),\n",
       " ('to', 30),\n",
       " ('unlike', 31),\n",
       " ('where', 32),\n",
       " ('you', 33)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_tokens = sorted(counter_vectorizer.vocabulary_.items(), key=lambda i:i[1])\n",
    "sorted_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a20fb79",
   "metadata": {},
   "source": [
    "1. `stemming` - chopping off the ends of words eg 'sandwiches' we remove the 'es' to remain with 'sandwich'\n",
    "2. `lemmatization` - improved version of stemming, it considers both pos and morphology of the word eg\n",
    "                    'ate' is changed to 'eat', 'better' is changed to 'good'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf382d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c959238",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a67d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_1 = \"SDM3ZJQ44F Confirmed.You have received Ksh50.00 from CAROLINE  ABUGA 0703992559 on 22/4/24 at 8:27 PM  New M-PESA balance is Ksh6,218.04. Use a unique M-PESA PIN to keep your money safe - don't use your date of birth as your PIN.\"\n",
    "\n",
    "transaction_2 = 'SDJ9MU5DIL Confirmed. On 19/4/24 at 8:57 AM Take Ksh1,100.00 cash from Christopher Rwara Your M-PESA float balance is Ksh97,007.00. Click the link to Download M-Pesa Agent App and Transact the SMART way https://bit.ly/3Ll6JQU'\n",
    "\n",
    "transaction_3 = 'SGO0PU3TBW Confirmed. on 24/7/24 at 7:37 PM Give Ksh1,000.00 to  ANTONE OKOTH MURING  New M-PESA float balance is  Ksh5,080.00. Click the link to Download M-Pesa Agent App and Transact the SMART way https://bit.ly/3Ll6JQU'\n",
    "\n",
    "transaction_4 = 'TEN8WJWHKI Confirmed. On 23/5/25 at 6:25 PM Take Ksh324.00 cash from Nancy Otieno Your M-PESA float balance is Ksh71,373.00. Click the link to Download M-Pesa Agent App and Transact the SMART way https://bit.ly/3Ll6JQU'\n",
    "\n",
    "transaction_5 = 'TDL9NHA8FV Confirmed. on 21/4/25 at 5:39 PM Give Ksh1,200.00 to  Paul kuria  New M-PESA float balance is  Ksh56,276.00. Click the link to Download M-Pesa Agent App and Transact the SMART way https://bit.ly/3Ll6JQU'\n",
    "\n",
    "transaction_6 = 'TDL2MXL1W8 Confirmed. On 21/4/25 at 3:37 PM Take Ksh814.00 cash from PERIS NGIGI Your M-PESA float balance is Ksh48,272.00. Click the link to Download M-Pesa Agent App and Transact the SMART way https://bit.ly/3Ll6JQU'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14bb6fd",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "| Tag  | Meaning              | Example           |\n",
    "|------|----------------------|-------------------|\n",
    "| NN   | Noun (singular)      | cat, time, torch  |\n",
    "| NNS  | Noun (plural)        | dogs, books       |\n",
    "| VB   | Verb (base form)     | run, eat          |\n",
    "| VBD  | Verb (past tense)    | ran, ate, had     |\n",
    "| VBG  | Verb (gerund)        | running, eating   |\n",
    "| JJ   | Adjective            | blue, fast        |\n",
    "| RB   | Adverb               | quickly, silently |\n",
    "| IN   | Preposition/Subord.  | in, on, because   |\n",
    "| DT   | Determiner           | a, the, an        |\n",
    "| PRP  | Personal pronoun     | I, you, they      |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cfb55c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xBase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
